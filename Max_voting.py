# -*- coding: utf-8 -*-
"""
Created on Tue Feb 16 10:36:36 2021

@author: Ankita Raghuvanshi
"""
  # for reproducibility
import pandas as pd
import numpy as np
import statistics 
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Activation,Dropout,Conv1D, Dense, Flatten
from keras.utils import  to_categorical
np.random.seed(1337)


from tensorflow.keras.layers.experimental import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,f1_score,classification_report,confusion_matrix
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Activation, LSTM, Dropout
from keras import optimizers
from keras.wrappers.scikit_learn import KerasClassifier
from dbn import SupervisedDBNClassification
from sklearn.datasets import make_classification
from sklearn.feature_selection import SelectKBest
from sklearn import metrics
from sklearn.feature_selection import f_classif
data = pd.read_csv(r"D:\project\feature_vectors_syscalls_frequency_5_Cat.csv")
data1 = data.iloc[:,:-1]
rows, cols = (11598, 139) 
data_norm = [[0]*cols]*rows 
mu = data1.mean()
test1 = data.iloc[748,:-1]
std_devv = data1.std()
data_norm = (data1-mu)/std_devv
d_n = data_norm.to_numpy()
y = data.iloc[:,139]
y1 = y.to_numpy()
# feature selection
def select_features(X_train, y_train):
	# configure to select all features
	fs = SelectKBest(score_func=f_classif, k=98)
	# learn relationship from training data
	fs.fit(X_train, y_train)
	# transform train input data
	X_train_fs = fs.transform(X_train)
	return X_train_fs, fs

# feature selection
X_train_fs, fs = select_features(data_norm, y1)
X_train, X_test, y_train, y_test = train_test_split(X_train_fs, y1, test_size=0.3, shuffle= True)
train_X = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
test_X = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
#CNN
model1 = Sequential()
model1.add(Conv1D(200,1,input_shape = (1,1,98),padding = 'Valid',activation= 'relu'))
model1.add(Dropout(0.2))
model1.add(Dense(100, activation='relu'))
model1.add(Flatten())
model1.add(Dense(6,activation = 'relu' ))
model1.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer= 'adam',
    metrics=["accuracy"],
)
#RNN
model2 = Sequential()
model2.add(LSTM(200,return_sequences=True,input_shape=(X_train.shape[0], X_train.shape[1]),activation = 'relu'))
model2.add(Dropout(0.35))
model2.add(LSTM(80))
model2.add(Dropout(0.45))
model2.add(Dense(6,activation='relu'))
model2.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer='adam',
    metrics=["accuracy"],
)


#DBN
model3 = SupervisedDBNClassification(hidden_layers_structure=[256, 256],
                                         learning_rate_rbm=0.00000015,
                                         learning_rate=0.3,
                                         n_epochs_rbm=3,
                                         n_iter_backprop=120,
                                         batch_size=128,
                                         activation_function='relu')

model1.fit(train_X,y_train,epochs=120, batch_size=128, validation_data=(test_X, y_test), verbose=2)
model2.fit(train_X,y_train,epochs=120, batch_size=128, validation_data=(test_X, y_test), verbose=2)
model3.fit(X_train,y_train)

pred1=model1.predict(test_X)
pred2=model2.predict(test_X)
pred3=model3.predict(X_test) 
op = np.argmax(pred1,axis=1)
op2 = np.argmax(pred2,axis=1)
final_pred = np.array([])
for i in range(0,len(X_test)):
    final_pred = np.append(final_pred, statistics.mode([op[i], op2[i], pred3[i]]))
    
print("Validation accuracy %f" %accuracy_score(y_test,final_pred))

pred11=model1.predict(train_X)
pred12=model2.predict(train_X)
pred13=model3.predict(X_train)
op1 = np.argmax(pred11,axis=1)
op12 = np.argmax(pred12,axis=1)
final_pred1 = np.array([])
for i in range(0,len(X_train)):
    final_pred1 = np.append(final_pred1, statistics.mode([op1[i], op12[i], pred13[i]]))
    
print("training accuracy %f" % accuracy_score(y_train,final_pred1))
y_pred = final_pred
print(classification_report(y_test, y_pred))
