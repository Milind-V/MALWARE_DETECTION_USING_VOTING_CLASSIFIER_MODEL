import pandas as pd
import numpy as np
import statistics
import tensorflow
from tensorflow import keras
from numpy import asarray
from numpy import savetxt
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from keras.models import Sequential
from keras.preprocessing.image import ImageDataGenerator
from keras.layers import BatchNormalization,Activation,Dropout
from keras.layers import Conv1D, MaxPooling2D, Dense, Flatten,Conv1D
from keras.utils import normalize, to_categorical
import tensorflow as tf
import numpy as np
import tensorflow as tf
from keras.regularizers import l2
from tensorflow import keras
from tensorflow.keras import layers,models,activations
from tensorflow.keras.layers.experimental import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,f1_score,classification_report
from keras.datasets import reuters
from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense, SimpleRNN, Activation, LSTM, Dropout
from keras import optimizers
from keras.wrappers.scikit_learn import KerasClassifier

from sklearn.datasets import make_classification
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
data = pd.read_csv(r"D:\project\feature_vectors_syscalls_frequency_5_Cat.csv")
data1 = data.iloc[:,:-1]
rows, cols = (11598, 139) 
data_norm = [[0]*cols]*rows 
mu = data1.mean()
test1 = data.iloc[748,:-1]
std_devv = data1.std()
data_norm = (data1-mu)/std_devv
d_n = data_norm.to_numpy()
y = data.iloc[:,139]
y1 = y.to_numpy()
# feature selection
def select_features(X_train, y_train):
	# configure to select all features
	fs = SelectKBest(score_func=f_classif, k=98)
	# learn relationship from training data
	fs.fit(X_train, y_train)
	# transform train input data
	X_train_fs = fs.transform(X_train)
	return X_train_fs, fs

# feature selection
X_train_fs, fs = select_features(data_norm, y1)
X_train, X_test, y_train, y_test = train_test_split(X_train_fs, y1, test_size=0.3, shuffle= True)
train_X = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
test_X = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
model1 = Sequential()
model1.add(Conv1D(200,1,input_shape = (1,1,98),padding = 'Valid',activation= 'relu'))
model1.add(Dropout(0.3))
model1.add(Dense(100, activation='relu'))
model1.add(Flatten())
model1.add(Dense(6,activation = 'relu' ))
model1.compile(
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
    optimizer= 'adam',
    metrics=["accuracy"],
)
history = model1.fit(train_X, y_train, epochs=120, batch_size=128, validation_data=(test_X, y_test), verbose=2)

label = model1.predict(test_X)

label1 = model1.predict(train_X)
op = np.argmax(label,axis=1)
ops =np.argmax(label1,axis=1)
 
print(f1_score(y_test, op,average = 'micro'))
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
y_pred = model1.predict_classes(test_X)
print(classification_report(y_test, y_pred))
